{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from model import ValueNetwork, SoftQNetwork, PolicyNetwork\n",
    "from util import ReplayBuffer, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Next we active GPU support if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Below cell will launch the unity environment for reacher..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Launch the environment\n",
    "env = UnityEnvironment(file_name='Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# Get environment info\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. We now load our neural networks, our agent and define our optimizers and criterions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = brain.vector_action_space_size\n",
    "state_dim  = brain.vector_observation_space_size\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net        = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "target_value_net = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "\n",
    "soft_q_net = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, device).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion = nn.MSELoss()\n",
    "\n",
    "value_lr  = 3e-4\n",
    "soft_q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "soft_q_optimizer = optim.Adam(soft_q_net.parameters(), lr=soft_q_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Let's define the number of episodes, steps and batch size for sampling from the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes  = 350\n",
    "max_steps   = 1000\n",
    "episode_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Below we define our soft q update rule as per the SAC paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_q_update(batch_size, \n",
    "           gamma=0.99,\n",
    "           mean_lambda=1e-3,\n",
    "           std_lambda=1e-3,\n",
    "           z_lambda=0.0,\n",
    "           soft_tau=1e-2,\n",
    "          ):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    expected_q_value = soft_q_net(state, action)\n",
    "    expected_value   = value_net(state)\n",
    "    new_action, log_prob, z, mean, log_std = policy_net.evaluate(state)\n",
    "\n",
    "\n",
    "    target_value = target_value_net(next_state)\n",
    "    next_q_value = reward + (1 - done) * gamma * target_value\n",
    "    q_value_loss = soft_q_criterion(expected_q_value, next_q_value.detach())\n",
    "\n",
    "    expected_new_q_value = soft_q_net(state, new_action)\n",
    "    next_value = expected_new_q_value - log_prob\n",
    "    value_loss = value_criterion(expected_value, next_value.detach())\n",
    "\n",
    "    log_prob_target = expected_new_q_value - expected_value\n",
    "    policy_loss = (log_prob * (log_prob - log_prob_target).detach()).mean()\n",
    "    \n",
    "\n",
    "    mean_loss = mean_lambda * mean.pow(2).mean()\n",
    "    std_loss  = std_lambda  * log_std.pow(2).mean()\n",
    "    z_loss    = z_lambda    * z.pow(2).sum(1).mean()\n",
    "\n",
    "    policy_loss += mean_loss + std_loss + z_loss\n",
    "\n",
    "    soft_q_optimizer.zero_grad()\n",
    "    q_value_loss.backward()\n",
    "    soft_q_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Now we let the agent learn by interacting with the environment and plotting the rewards..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAE/CAYAAACuHMMLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFlhJREFUeJzt3X+0XWV95/H3RyKIv/gtAgGCgrWhFXRuQafaUkEMKsZS1gxUZ9JZdGhXZarVLgt1RhTtWtIfop1qa6pUxlpFotWoUymiMF2tIjeA1YCY8GslCBINP0xREfnOH+cJPVxvcm9yD5zcPO/XWmedvZ/9nL2/z8nhs/d99rmXVBWSpL48btwFSJIee4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH89JpKsTnLciPf5oSTvGOU+d2ZJjkuyftx1aMdg+OsxUVVHVtUV465jWJLXJbklyb8luSHJs4a2/XqS29q2TyXZe5y1jkOSRUm+lOT+JN9McsJW+u6W5MIk9yW5M8kbHstate0Mf3UpyW8CZwAvB54MvAL4btt2JPB+4L8A+wP3A+/bzuMsGEW9YzruR4FrgX2ANwMrkuy3hb5vBY4ADgV+BXhTkiUjqEGPEsNfs5bkwCSfSLKhXTH/7tC2tyZZkeTiJN9Pck2So4a237r5yjHJMUkm21Xid5K8a6jfK9sU0T1Jrkjys0Pbntv2+/0kFwNPmFLfK5Jc1177L0mes4VxPA44F/i9qrq+Bm6qqo2ty6uBz1TV/6uqTcD/Ak5J8pRZvEfHJVmf5A+S3An8zdZqS/Lfknxm6PVrklwytL4uydFt+T1t/b4kq5K8aJr3/2+T3Af8RpLd29TY3UmuB35hpvqH9vcs4HnAuVX1g6r6BPB14Ne28JJlwNur6u6qugH4a+A3Zns8PfYMf81KC8zPAF8DDgKOB16f5KVD3ZYClwB7A38HfCrJ46fZ3XuA91TVU4FnAh9vx3gWg6vN1wP7Af8X+EySXZPsCnwK+HDb/yUMBVGS5wIXAr/F4Er1/cDKJLtNc/yF7fFzLUxvSfK2NkaAI9s4Aaiqm4AHgGf99K6m9fRW46HAmTPUdiXwoiSPS3IgsCvwgjamZzD4qeRf236vBo7m39/fS5IMnwCXAiuAPYGPMDjBPbM9XsogoB+W5H1JtvQTzZHAzVX1/aG2r7X2R0iyF3AAQ+/Zlvpqx2H4a7Z+Adivqs6rqgeq6mYGV3enDfVZVVUrqurHwLsYXJk/f5p9/Rg4PMm+VbWpqr7S2v8z8Lmquqzt40+B3YH/2PbzeODdVfXjqlrBIAw3OxN4f1VdVVU/qaqLgB9t4fgL2/OJwM8zmKY4ncE0EAwC994pr7kXmPHKv3mIwRXzj6rqB1urrb2P32cQ6r8EXAp8O8mzgV8G/qmqHgKoqr+tqu9V1YNV9WfAbsDPDB33y1X1qap6qB33PwF/VFUbq2od8OfDRVbV71TV72xhDNvyHjx5aPtMfbWDMPw1W4cCB7Zpi3uS3AP8IYM58c3WbV5ogbUeOHCafZ3B4Cr6m0muTvKK1n4gcNuUfaxj8JPGgcDt9ci/RHjb0PKhwBun1HfwFo7/g/b8x1V1T1XdyuBq/GWtfRPw1CmveSqDkJ6NDVX1w22o7UrgOAbhfyVwBYPg/+W2DkCS3283pu9t+9gD2HfoOOt4pAOntN3G7G3Le7BpaPtMfbWDMPw1W+uAW6pqz6HHU6rqZUN9Dt680KZQFgLfnrqjqlpTVacDTwPOZ3Aj8Umt76FD+0jb5+3AHcBBrW2zQ6bU90dT6ntiVX10mrHcyGAaZ/hEMry8Ghi+X/EMBlfZ35pmX9OZ+qdyZ6ptc/i/qC1fyZTwb/P7b2JwNb9XVe3J4Op6+P2Yetw7GPo34ZHv10xWA8+Ycp/jqNb+CFV1dzvWUTP11Y7D8NdsfRX4fruRuXuSXZL8XJLhm4j/Ickp7Zsmr2cwtfGVqTtK8pok+7Ur+3ta80MM5v5fnuT4dq/gjW0f/wJ8GXgQ+N0kj09yCnDM0G7/GvjtJMdm4ElJXj7dTdqquh+4mME3Up6SZCGDqZnPti4fAU5O8qJ2UjoP+OSU+e9tMVNtVzKYetq9qtYD/wQsYXB/4NrW5ylt/BuABUnewk9fmU/1ceCcJHu1Mf6P2RZcVd8CrgPOTfKEJL8KPAf4xBZe8n+A/9mO9WzgvwMfmu3x9Ngz/DUrVfUTBl+HPBq4hcHXIj/AYOphs08zmLe/m8HXJE9pc/dTLQFWJ9nE4Obvae0bJTcCrwH+d9v/ycDJ7R7DA8ApDL5BsrEd55ND9U0yCJy/aMdfy9a/bXIWg+mKbzM4sfwdg5uyVNVq4LcZnATuYhC8D8+NJ/mHJH+4lX0/wky1taDdxCD0qar7gJuBf27vOwzuBXyewU8ftwE/5KeneaZ6W+t7C/CPDG6WPyzJXyX5q628/jRgotX8TuDUqtrQXvvqJMNX9ucCN7XjXQn8SVV9fob6NEbxf+aiUUjyVuDwqnrNuGuRNDOv/CWpQ4a/JHXIaR9J6pBX/pLUIcNfkjo0lr84OFf77rtvLVq0aNxlSNIOZ9WqVd+tqi399dWHzcvwX7RoEZOTk+MuQ5J2OElm9Wc8nPaRpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDIwn/JEuS3JhkbZKzp9m+W5KL2/arkiyasv2QJJuS/P4o6pEkbd2cwz/JLsB7gZOAxcDpSRZP6XYGcHdVHQ5cAJw/Zfu7gH+Yay2SpNkZxZX/McDaqrq5qh4APgYsndJnKXBRW14BHJ8kAEleBdwCrB5BLZKkWRhF+B8ErBtaX9/apu1TVQ8C9wL7JHky8AfA20ZQhyRplsZ9w/etwAVVtWmmjknOTDKZZHLDhg2PfmWStBNbMIJ93A4cPLS+sLVN12d9kgXAHsD3gGOBU5P8MbAn8FCSH1bVX0w9SFUtB5YDTExM1AjqlqRujSL8rwaOSHIYg5A/Dfj1KX1WAsuALwOnAl+sqgJetLlDkrcCm6YLfknSaM05/KvqwSRnAZcCuwAXVtXqJOcBk1W1Evgg8OEka4GNDE4QkqQxyeACfH6ZmJioycnJcZchSTucJKuqamKmfuO+4StJGgPDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUoZGEf5IlSW5MsjbJ2dNs3y3JxW37VUkWtfaXJFmV5Ovt+cWjqEeStHVzDv8kuwDvBU4CFgOnJ1k8pdsZwN1VdThwAXB+a/8ucHJV/TywDPjwXOuRJM1sFFf+xwBrq+rmqnoA+BiwdEqfpcBFbXkFcHySVNW1VfXt1r4a2D3JbiOoSZK0FaMI/4OAdUPr61vbtH2q6kHgXmCfKX1+Dbimqn403UGSnJlkMsnkhg0bRlC2JPVrh7jhm+RIBlNBv7WlPlW1vKomqmpiv/32e+yKk6Sd0CjC/3bg4KH1ha1t2j5JFgB7AN9r6wuBvwf+a1XdNIJ6JEkzGEX4Xw0ckeSwJLsCpwErp/RZyeCGLsCpwBerqpLsCXwOOLuq/nkEtUiSZmHO4d/m8M8CLgVuAD5eVauTnJfkla3bB4F9kqwF3gBs/jroWcDhwFuSXNceT5trTZKkrUtVjbuGbTYxMVGTk5PjLkOSdjhJVlXVxEz9dogbvpKkx5bhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUodGEv5JliS5McnaJGdPs323JBe37VclWTS07ZzWfmOSl46iHknS1s05/JPsArwXOAlYDJyeZPGUbmcAd1fV4cAFwPnttYuB04AjgSXA+9r+JEmPolFc+R8DrK2qm6vqAeBjwNIpfZYCF7XlFcDxSdLaP1ZVP6qqW4C1bX+SpEfRghHs4yBg3dD6euDYLfWpqgeT3Avs09q/MuW1B42gpmm97TOruf7b9z1au5ekkVh84FM59+QjH9VjzJsbvknOTDKZZHLDhg3jLkeS5rVRXPnfDhw8tL6wtU3XZ32SBcAewPdm+VoAqmo5sBxgYmKitqfQR/tMKknzxSiu/K8GjkhyWJJdGdzAXTmlz0pgWVs+FfhiVVVrP619G+gw4AjgqyOoSZK0FXO+8m9z+GcBlwK7ABdW1eok5wGTVbUS+CDw4SRrgY0MThC0fh8HrgceBF5bVT+Za02SpK3L4AJ8fpmYmKjJyclxlyFJO5wkq6pqYqZ+8+aGryRpdAx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KH5hT+SfZOclmSNe15ry30W9b6rEmyrLU9Mcnnknwzyeok75xLLZKk2Zvrlf/ZwOVVdQRweVt/hCR7A+cCxwLHAOcOnST+tKqeDTwX+MUkJ82xHknSLMw1/JcCF7Xli4BXTdPnpcBlVbWxqu4GLgOWVNX9VfUlgKp6ALgGWDjHeiRJszDX8N+/qu5oy3cC+0/T5yBg3dD6+tb2sCR7Aicz+OlhWknOTDKZZHLDhg1zq1qSOrdgpg5JvgA8fZpNbx5eqapKUttaQJIFwEeBP6+qm7fUr6qWA8sBJiYmtvk4kqR/N2P4V9UJW9qW5DtJDqiqO5IcANw1TbfbgeOG1hcCVwytLwfWVNW7Z1WxJGnO5jrtsxJY1paXAZ+eps+lwIlJ9mo3ek9sbSR5B7AH8Po51iFJ2gZzDf93Ai9JsgY4oa2TZCLJBwCqaiPwduDq9jivqjYmWchg6mgxcE2S65L85hzrkSTNQqrm3/T5xMRETU5OjrsMSdrhJFlVVRMz9fM3fCWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHZpT+CfZO8llSda057220G9Z67MmybJptq9M8o251CJJmr25XvmfDVxeVUcAl7f1R0iyN3AucCxwDHDu8EkiySnApjnWIUnaBnMN/6XARW35IuBV0/R5KXBZVW2sqruBy4AlAEmeDLwBeMcc65AkbYO5hv/+VXVHW74T2H+aPgcB64bW17c2gLcDfwbcP8c6JEnbYMFMHZJ8AXj6NJvePLxSVZWkZnvgJEcDz6yq30uyaBb9zwTOBDjkkENmexhJ0jRmDP+qOmFL25J8J8kBVXVHkgOAu6bpdjtw3ND6QuAK4AXARJJbWx1PS3JFVR3HNKpqObAcYGJiYtYnGUnST5vrtM9KYPO3d5YBn56mz6XAiUn2ajd6TwQuraq/rKoDq2oR8ELgW1sKfknSaM01/N8JvCTJGuCEtk6SiSQfAKiqjQzm9q9uj/NamyRpTFI1/2ZQJiYmanJyctxlSNIOJ8mqqpqYqZ+/4StJHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqkOEvSR0y/CWpQ4a/JHXI8JekDhn+ktQhw1+SOmT4S1KHDH9J6pDhL0kdMvwlqUOGvyR1yPCXpA4Z/pLUIcNfkjpk+EtShwx/SeqQ4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUocMf0nqUKpq3DVssyQbgNu28+X7At8dYTnjtDONBXau8TiWHdfONJ7pxnJoVe030wvnZfjPRZLJqpoYdx2jsDONBXau8TiWHdfONJ65jMVpH0nqkOEvSR3qMfyXj7uAEdqZxgI713gcy45rZxrPdo+luzl/SVKfV/6S1L1uwj/JkiQ3Jlmb5Oxx17OtklyY5K4k3xhq2zvJZUnWtOe9xlnjbCU5OMmXklyfZHWS17X2eTeeJE9I8tUkX2tjeVtrPyzJVe3zdnGSXcdd67ZIskuSa5N8tq3Py/EkuTXJ15Ncl2Sytc27z9lmSfZMsiLJN5PckOQF2zueLsI/yS7Ae4GTgMXA6UkWj7eqbfYhYMmUtrOBy6vqCODytj4fPAi8saoWA88HXtv+PebjeH4EvLiqjgKOBpYkeT5wPnBBVR0O3A2cMcYat8frgBuG1ufzeH6lqo4e+krkfPycbfYe4PNV9WzgKAb/Rts3nqra6R/AC4BLh9bPAc4Zd13bMY5FwDeG1m8EDmjLBwA3jrvG7RzXp4GXzPfxAE8ErgGOZfCLNwta+yM+fzv6A1jYQuTFwGeBzNfxALcC+05pm5efM2AP4Bbavdq5jqeLK3/gIGDd0Pr61jbf7V9Vd7TlO4H9x1nM9kiyCHgucBXzdDxtiuQ64C7gMuAm4J6qerB1mW+ft3cDbwIeauv7MH/HU8A/JlmV5MzWNi8/Z8BhwAbgb9qU3AeSPIntHE8v4b/Tq8Fpf159dSvJk4FPAK+vqvuGt82n8VTVT6rqaAZXzMcAzx5zSdstySuAu6pq1bhrGZEXVtXzGEz5vjbJLw1vnE+fM2AB8DzgL6vqucC/MWWKZ1vG00v43w4cPLS+sLXNd99JcgBAe75rzPXMWpLHMwj+j1TVJ1vzvB0PQFXdA3yJwbTInkkWtE3z6fP2i8Ark9wKfIzB1M97mKfjqarb2/NdwN8zODnP18/ZemB9VV3V1lcwOBls13h6Cf+rgSPaNxZ2BU4DVo65plFYCSxry8sYzJ3v8JIE+CBwQ1W9a2jTvBtPkv2S7NmWd2dw7+IGBieBU1u3eTEWgKo6p6oWVtUiBv+dfLGqXs08HE+SJyV5yuZl4ETgG8zDzxlAVd0JrEvyM63peOB6tnc8476J8RjeLHkZ8C0G87FvHnc921H/R4E7gB8zuAI4g8Fc7OXAGuALwN7jrnOWY3khgx9N/xW4rj1eNh/HAzwHuLaN5RvAW1r7M4CvAmuBS4Ddxl3rdoztOOCz83U8reavtcfqzf/dz8fP2dCYjgYm2+ftU8Be2zsef8NXkjrUy7SPJGmI4S9JHTL8JalDhr8kdcjwl6QOGf6S1CHDX5I6ZPhLUof+P1CMBMvAQFSCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "input and target shapes do not match: input [64 x 1], target [64 x 64 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/THNN/generic/MSECriterion.c:13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e0260dae36b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0msoft_q_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a2220f99dac0>\u001b[0m in \u001b[0;36msoft_q_update\u001b[0;34m(batch_size, gamma, mean_lambda, std_lambda, z_lambda, soft_tau)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtarget_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_value_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnext_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mq_value_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_q_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_q_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_q_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mexpected_new_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_q_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \"\"\"\n\u001b[1;32m   1568\u001b[0m     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss,\n\u001b[0;32m-> 1569\u001b[0;31m                            input, target, size_average, reduce)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pointwise_loss\u001b[0;34m(lambd, lambd_optimized, input, target, size_average, reduce)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlambd_optimized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and target shapes do not match: input [64 x 1], target [64 x 64 x 1] at /Users/soumith/code/builder/wheel/pytorch-src/aten/src/THNN/generic/MSECriterion.c:13"
     ]
    }
   ],
   "source": [
    "while episode_idx < max_episodes:\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = policy_net.get_action(state, device)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[-1]\n",
    "        done = env_info.local_done\n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            soft_q_update(batch_size)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        episode_idx += 1\n",
    "        \n",
    "        if episode_idx % 10 == 0:\n",
    "            plot(episode_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
